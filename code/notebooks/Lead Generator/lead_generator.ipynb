{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import web scraping libraries\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "from IPython.display import clear_output, display\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "leads_db = []\n",
        "\n",
        "lead = {\n",
        "    'uid': '1234567890',\n",
        "    # 'name': 'John Doe',\n",
        "    # 'email': '',\n",
        "    # 'phone': '123-456-7890',\n",
        "    # 'company': 'ABC Company',\n",
        "    # 'title': 'CEO',\n",
        "    # 'location': 'New York, NY',\n",
        "    # 'website': 'www.abc.com',\n",
        "    # 'linkedin': 'www.linkedin.com/in/johndoe',\n",
        "    # 'twitter': 'www.twitter.com/johndoe',\n",
        "    # 'snapchat': 'www.snapchat.com/johndoe',\n",
        "    # 'twitch': 'www.twitch.com/johndoe',\n",
        "    # 'tumblr': 'www.tumblr.com/johndoe',\n",
        "    # 'reddit': 'www.reddit.com/johndoe',\n",
        "    # 'quora': 'www.quora.com/johndoe',\n",
        "    # 'medium': 'www.medium.com/johndoe',\n",
        "    # 'github': 'www.github.com/johndoe',\n",
        "    # 'angel': 'www.angel.co/johndoe',\n",
        "    # 'crunchbase': 'www.crunchbase.com/johndoe',\n",
        "    # 'producthunt': 'www.producthunt.com/johndoe',\n",
        "    # 'behance': 'www.behance.com/johndoe',\n",
        "    # 'dribbble': 'www.dribbble.com/johndoe',\n",
        "    # 'flickr': 'www.flickr.com/johndoe',\n",
        "    # 'country': 'United States',\n",
        "    # 'state': 'New York',\n",
        "    # 'city': 'New York',\n",
        "    # 'zip': '10001',\n",
        "    # 'industry': '',\n",
        "    # 'revenue': '',\n",
        "    # 'employees': '',\n",
        "    # 'tags': '',\n",
        "    # 'notes': '',\n",
        "    'status': 'New',\n",
        "    'date': datetime.datetime.now(),\n",
        "    # 'assigned': 'John Doe',\n",
        "    # 'owner': 'John Doe',\n",
        "    'lead_score': 0,\n",
        "    'lead_score_reason': '',\n",
        "    'lead_score_date': datetime.datetime.now(),\n",
        "    'lead_source': '',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "FILTERED_ORGS_KEYWORDS = ['securities', 'sec', 'court', 'fbi', 'ponzi', 'llc', 'office', 'authority', 'act', \n",
        "    'commission', 'department', 'federal', 'government', 'law', 'legal', 'police', 'state', 'united', 'states', 'cyber']\n",
        "\n",
        "WHITE_LIST_KEYWORDS = ['fraud', 'corruption', 'bribe', 'compliance']\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Base Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write a function to check if a web page exists\n",
        "def is_active_url(url):\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        if r.status_code == 200:\n",
        "            return True\n",
        "    except:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "# download the web page and store it in a file as html\n",
        "def download_page(url, file_name):\n",
        "    try:\n",
        "        # check if the url exists\n",
        "        if is_active_url(url):\n",
        "            r = requests.get(url)\n",
        "            # save web page to data folder\n",
        "            with open(file_name.format(url.split('/')[-1]), 'w') as f:\n",
        "                f.write(r.text)\n",
        "            # print('Downloaded', url, 'at', file_name)\n",
        "            return True\n",
        "        else:\n",
        "            print('URL not active', url)\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print('Error downloading page', url)\n",
        "        print(e)\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "# create a function to write leads to csv file\n",
        "def write_leads_to_csv(leads):\n",
        "    # check if leads is empty\n",
        "    if len(leads) == 0:\n",
        "        return\n",
        "\n",
        "    # write leads to csv file\n",
        "    for lead in leads:\n",
        "        with open('data/leads.csv', 'a') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=lead.keys())\n",
        "            # check if file is empty\n",
        "            if f.tell() == 0:\n",
        "                # write header\n",
        "                writer.writeheader()\n",
        "            \n",
        "            for lead in leads:\n",
        "                # write lead\n",
        "                writer.writerow(lead)\n",
        "\n",
        "    # remove duplicates from csv file\n",
        "    df = pd.read_csv('data/leads.csv')\n",
        "    df.drop_duplicates(subset=['uid', 'company'], inplace=True)\n",
        "    df.to_csv('data/leads.csv', index=False)\n",
        "\n",
        "    \n",
        "\n",
        "# create a function to create leads\n",
        "def lead_creation(uid, source, text, filtered_orgs=FILTERED_ORGS_KEYWORDS):\n",
        "    # create a spacy doc object \n",
        "    doc = nlp(text)\n",
        " \n",
        "    # create a list of entities\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
        "\n",
        "    invalid_orgs = []\n",
        "    # check if any of the words in the entities match with words in invalid_orgs\n",
        "    for entity in entities:\n",
        "        for word in entity.split():\n",
        "            if word.lower() in filtered_orgs:\n",
        "                invalid_orgs.append(entity)\n",
        "                break\n",
        "\n",
        "    # get intersection of entities and invalid_orgs\n",
        "    valid_orgs = list(set(entities) - set(invalid_orgs))\n",
        "\n",
        "    leads = []\n",
        "\n",
        "    # if there are no valid orgs, return empty string\n",
        "    if len(valid_orgs) > 0:\n",
        "        for org in valid_orgs:\n",
        "            # create a new leads\n",
        "            new_lead = lead.copy()\n",
        "            new_lead['uid'] = uid\n",
        "            new_lead['company'] = org\n",
        "            new_lead['lead_source'] = source\n",
        "            new_lead['notes'] = ''\n",
        "            leads.append(new_lead)\n",
        "\n",
        "    return leads\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Country Functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## United States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select web page elements to scrape\n",
        "def scrape_page_us_sec(file_name):\n",
        "    # read the html file\n",
        "    with open(file_name, 'r') as f:\n",
        "        html = f.read()\n",
        "\n",
        "    # create a soup object\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # use beautifulsoup to select elements with a selector\n",
        "    # select all the divs with id = 'mainlist'\n",
        "    content = soup.select('#mainlist')\n",
        "\n",
        "    # check if content exists\n",
        "    if len(content) > 0:\n",
        "        # find total rows in the table\n",
        "        total_rows = len(content[0].select('tr'))\n",
        "\n",
        "        # select each table row of role as row\n",
        "        for row in tqdm(content[0].select('tr'), total=total_rows, desc='Scraping'):\n",
        "            # check if first column of the row has an a element\n",
        "            if len(row.select('td a')) > 0:\n",
        "                # select the first a element\n",
        "                a = row.select('td a')[0]\n",
        "                # get the text of the a element\n",
        "                case_number = a.text\n",
        "                # get the href attribute of the a element\n",
        "                case_url = a['href']\n",
        "\n",
        "                # create full url from the href attribute\n",
        "                full_url = 'https://www.sec.gov' + str(case_url)\n",
        "\n",
        "                # check if the url exists\n",
        "                if is_active_url(full_url):\n",
        "                    # download the web page\n",
        "                    download_page(full_url, file_name=f'temp/{case_number}.html')\n",
        "                    \n",
        "                    # wait for 1 second\n",
        "                    time.sleep(1)\n",
        "\n",
        "                    # clear the output\n",
        "                    # clear_output(wait=True)\n",
        "\n",
        "                    # extract all paragraphs from the web page under the div with class = 'alpha'\n",
        "                    with open(f'temp/{case_number}.html', 'r') as f:\n",
        "                        html = f.read()\n",
        "                    soup = BeautifulSoup(html, 'html.parser')\n",
        "                    content = soup.select('.alpha')\n",
        "                    p_texts = [p.text for p in content[0].select('p')]\n",
        "                    p_text = ' '.join(p_texts)\n",
        "\n",
        "                    # create leads\n",
        "                    leads = lead_creation(case_number, full_url, p_text)\n",
        "\n",
        "                    # write leads to csv file\n",
        "                    write_leads_to_csv(leads)\n",
        "\n",
        "                else:\n",
        "                    print('URL not active', full_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scrape Leads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Location</th>\n",
              "      <th>URL</th>\n",
              "      <th>Agency</th>\n",
              "      <th>Active</th>\n",
              "      <th>Reviews URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>United States</td>\n",
              "      <td>https://www.sec.gov/</td>\n",
              "      <td>Securities and Exchange Commission (SEC)</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.sec.gov/litigation/litreleases.htm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>European Union</td>\n",
              "      <td>https://www.esma.europa.eu/</td>\n",
              "      <td>European Securities and Markets Authority (ESMA)</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>https://www.fca.org.uk/</td>\n",
              "      <td>Financial Conduct Authority (FCA)</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Canada</td>\n",
              "      <td>https://www.securities</td>\n",
              "      <td>Canadian Securities Administrators (CSA)</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Australia</td>\n",
              "      <td>https://www.asic.gov.au/</td>\n",
              "      <td>Australian Securities and Investments Commissi...</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Location                          URL  \\\n",
              "0   United States         https://www.sec.gov/   \n",
              "1  European Union  https://www.esma.europa.eu/   \n",
              "2  United Kingdom      https://www.fca.org.uk/   \n",
              "3          Canada       https://www.securities   \n",
              "4       Australia     https://www.asic.gov.au/   \n",
              "\n",
              "                                              Agency  Active  \\\n",
              "0           Securities and Exchange Commission (SEC)    True   \n",
              "1   European Securities and Markets Authority (ESMA)   False   \n",
              "2                  Financial Conduct Authority (FCA)   False   \n",
              "3           Canadian Securities Administrators (CSA)   False   \n",
              "4  Australian Securities and Investments Commissi...   False   \n",
              "\n",
              "                                      Reviews URL  \n",
              "0  https://www.sec.gov/litigation/litreleases.htm  \n",
              "1                                             NaN  \n",
              "2                                             NaN  \n",
              "3                                             NaN  \n",
              "4                                             NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:45<00:00,  1.82s/it]cy/s]\n",
            "Agencies: 100%|\u001b[32m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 69/69 [00:46<00:00,  1.50 agency/s]\n"
          ]
        }
      ],
      "source": [
        "# read excel file in pandas\n",
        "df = pd.read_excel(\"data/agency_list.xlsx\")\n",
        "display(df.head())\n",
        "\n",
        "# loop through the excel file and check active urls\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc='Agencies', unit=' agency', leave=True, colour='green'):\n",
        "    if row['Active']:\n",
        "        location_name = row['Location'].strip().lower()\n",
        "\n",
        "        # generate a unique hash for the location\n",
        "        location_hash = hash(location_name)\n",
        "\n",
        "        # output file name\n",
        "        file_name = f'temp/{location_hash}.html'\n",
        "\n",
        "        # URL to scrape\n",
        "        url = row['Reviews URL']\n",
        "\n",
        "        # # download the web page\n",
        "        result = download_page(url, file_name=file_name)\n",
        "\n",
        "        if result:\n",
        "            scrape_page_us_sec(file_name=file_name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (main, Nov 25 2022, 10:49:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ba99acdf117cee92b1f5efc673e05521c59d4c7954d9f443fe7d6ffcba907025"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
